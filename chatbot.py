from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI

# db_manager.py íŒŒì¼ì—ì„œ í•¨ìˆ˜ë¥¼ import í•©ë‹ˆë‹¤.
from db_manager import load_or_create_vector_db


class RAGChatbot:
    def __init__(self, knowledge_file_path, db_persist_directory="my_vector_db"):
        load_dotenv()

        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", streaming=True)

        # DB
        self.vectorstore = load_or_create_vector_db(
            knowledge_file_path, db_persist_directory
        )

        self.retriever = self.vectorstore.as_retriever()
        self.prompt = self._create_prompt_template()
        self.rag_chain = self._create_rag_chain()
        self.chat_history = []
        print("ì±—ë´‡ì…ë‹ˆë‹¤~.")

    def _create_prompt_template(self):
        system_prompt = """
        ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
        ì£¼ì–´ì§„ ë‚´ìš©ì—ì„œë§Œ ë‹µë³€ì„ ì°¾ì•„ì•¼ í•˜ë©°, ë‚´ìš©ì„ ì§€ì–´ë‚´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

        ë‚´ìš©:
        {context}
        """
        return ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{question}"),
            ]
        )

    def _create_rag_chain(self):
        return (
            RunnablePassthrough.assign(
                context=lambda x: self.retriever.invoke(x["question"])
            )
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def ask(self, query):
        response_stream = self.rag_chain.stream(
            {"question": query, "chat_history": self.chat_history}
        )

        full_answer = ""
        print("ë‹µë³€: ", end="", flush=True)
        for chunk in response_stream:
            print(chunk, end="", flush=True)
            full_answer += chunk

        self.chat_history.append(HumanMessage(content=query))
        self.chat_history.append(AIMessage(content=full_answer))

    def start_chat(self):
        print("\n" + "=" * 30)
        print("ğŸ—£ï¸ ì±—ë´‡ê³¼ì˜ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        print("ì±—ë´‡ì„ ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 30)

        while True:
            query = input("ì§ˆë¬¸: ")
            if query.lower() == "exit":
                print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            self.ask(query)
            print("\n" + "-" * 50)
